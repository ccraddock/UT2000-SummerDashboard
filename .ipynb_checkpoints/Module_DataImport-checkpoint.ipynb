{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import Module\n",
    "The purpose of this notebook is to act as a module that can be imported into the Master Notebook to read in various data depending on the data acquisition source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Environmental Quality Data\n",
    "Used for import of:\n",
    "- Temperature\n",
    "- Relative Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ieqImport(student_no, directory, name_list, column_list='all', starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - deployment_info: a list of strings that contains the semester, deployment tag, deployment number,\n",
    "            starting date, and ending date of deployment\n",
    "        - beiwe_ids: a list of the beiwe IDs from this data set\n",
    "        - beacons: a list of the beacons from this data set\n",
    "        - directory: the final directory where the data is stored\n",
    "        - name_list: a list of strings for the columns to be used in the dataframe\n",
    "        - column_list: a list of numbers that correspond to the columns to be imported, default is 'all'\n",
    "    Imports different IEQ data variables and returns a Series object that contains dataframes for the specified IEQ variable\n",
    "    '''\n",
    "    # Important variables\n",
    "    df = pd.DataFrame() # Dataframe to return\n",
    "    start_date = datetime.strptime(starting, '%m/%d/%Y')\n",
    "    end_date = datetime.strptime(ending, '%m/%d/%Y')\n",
    "    \n",
    "    # Importing Data\n",
    "    for i in range(len(beiwe_ids)):\n",
    "        id_name = beiwe_ids[i]\n",
    "        ## Adding a leading zero to numbers < 10\n",
    "        if int(beacons[i]) < 10:\n",
    "            beacon_no = '0' + str(beacons[i])\n",
    "        else:\n",
    "            beacon_no = str(beacons[i])\n",
    "\n",
    "        ## Output for visual confirmation\n",
    "        print('\\nReading for Beacon: ' + str(beacon_no))\n",
    "        print('Reading for Beiwe ID: ' + str(id_name))\n",
    "        \n",
    "        ## Location of file\n",
    "        DIR = str(deployment_info[0]) + '/' + str(deployment_info[1]) + '/Beacon_Data/beacon-d' + str(deployment_info[2]) + '-' + str(beacon_no) +'/bevo/' + directory + '/'\n",
    "\n",
    "        temp = pd.DataFrame() # Stores one csv file's worth of data\n",
    "        raw_data = pd.DataFrame() # Appends each data file together\n",
    "        ## Looping through all the files in the sht31d directory\n",
    "        for file in os.listdir(DIR):\n",
    "            if str(file[-3:]) == 'csv': # To ensure that we only read in the csv files\n",
    "                if column_list == 'all': # if no column numbers are specified, the default is to read them all\n",
    "                    try:\n",
    "                        temp = pd.read_csv(DIR + file,header=None,names=name_list)\n",
    "                    except FileNotFoundError:\n",
    "                        print('No file found - wrong path')\n",
    "                else:\n",
    "                    try:\n",
    "                        temp = pd.read_csv(DIR + file,header=None,names=name_list,usecols=column_list)\n",
    "                    except FileNotFoundError:\n",
    "                        print('No file found - wrong path')\n",
    "                raw_data = pd.concat([raw_data,temp],axis=0,ignore_index=True)\n",
    "\n",
    "                ### Getting the file size\n",
    "                file_size += os.path.getsize(DIR + file)\n",
    "            \n",
    "        ## Creating a date array for indexing that converts utctimestamp to Central Time\n",
    "        raw_data = raw_data.dropna() # Dropping any NaNs\n",
    "        t = np.zeros((len(raw_data)),dtype='datetime64[ns]') # Array to store times\n",
    "        for j in range(len(t)):\n",
    "            ts = int(raw_data['time'].values[j])\n",
    "            ## Converting from UTC to specified format\n",
    "            t[j] = datetime.strptime(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S') - timedelta(hours=5)\n",
    "\n",
    "        ## Re-indexing and re-naming\n",
    "        raw_data['time_index'] = t\n",
    "        raw_data = raw_data.set_index('time_index')\n",
    "        raw_data = raw_data.sort_index()\n",
    "        raw_data = raw_data.drop(['time'],axis=1)\n",
    "        \n",
    "        ## Removing data from DF that isn't in the deployment range\n",
    "        ### Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ### Looping through all values read in\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                #### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "        ###  Removing the excess 'head' of data\n",
    "        raw_data = raw_data[start_index:]\n",
    "\n",
    "        ### Looping through all values read in (minus the part we just got rid of)\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == end_date.month and raw_data.index[j].day == end_date.day:\n",
    "                ### Once we find the end month and date, we want to stop and store the index just before this because the last day is incomplete\n",
    "                end_index = j-1\n",
    "                break\n",
    "        ### Removing the excess 'tail' of data\n",
    "        raw_data = raw_data[0:end_index]\n",
    "\n",
    "        ### Output of data length\n",
    "        if len(raw_data) < 0:\n",
    "            print('No data from this deployment')\n",
    "        else:\n",
    "            print('Number of datapoints: ' + str(len(raw_data)))\n",
    "\n",
    "        ## Storing the cleaned data to the final dataframe\n",
    "        df_byID[id_name] = raw_data\n",
    "        \n",
    "    # Returning dataframe with cleaned data\n",
    "    return df_byID, file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitbit Data\n",
    "Used for import of:\n",
    "- Step Count (hourly and daily)\n",
    "- Sleep Stages\n",
    "- Calories Burned (hourly and daily)\n",
    "- Intensities (hourly and daily)\n",
    "- Metabolic Equivalents (METs)\n",
    "- Heart Rate (1-minute and 15-minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitbitImport(deployment_info, beiwe_ids, record_ids, file_name, name_list, daily=True):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - deployment_info: a list of strings that contains the semester, deployment tag, deployment number,\n",
    "            starting date, and ending date of deployment\n",
    "        - beiwe_ids: a list of string containing the beiwe IDs from this deployment's data set\n",
    "        - record_ids: a list of strings containing the Fitbit IDs from this deployment's data set\n",
    "        - file_name: a string that contains the file name we are looking for\n",
    "        - name_list: a list of strings for the columns to be used in the dataframe\n",
    "        - interval: a string that specifies whether data is hourly or daily\n",
    "    Imports different Fitbit data variables and returns a Series object that contains dataframes for the specified Fitbit variable\n",
    "    '''\n",
    "    # Important variables\n",
    "    df_byID = pd.Series() # Series to return\n",
    "    start_date = deployment_info[3]\n",
    "    end_date = deployment_info[4]\n",
    "    DIR = deployment_info[0] + '/' + deployment_info[1] + '/Fitbit_Data/' # Directory where the file should be located\n",
    "    \n",
    "    # Importing the data from file\n",
    "    try:\n",
    "        raw_data = pd.read_csv(DIR + file_name, header=0, names=name_list)\n",
    "    except FileNotFoundError:\n",
    "        print('No file found - wrong path')\n",
    "\n",
    "    # Parsing out data by Fitbit ID\n",
    "    ids = [raw_data.iloc[0]['ID']] # variable to store the various ids in the file\n",
    "    ## Getting the IDs\n",
    "    for i in range(len(raw_data)-1):\n",
    "        if raw_data.iloc[i]['ID'] != raw_data.iloc[i+1]['ID']:\n",
    "            ids.append(raw_data.iloc[i+1]['ID'])\n",
    "            \n",
    "    ## Converting the time column to datetime\n",
    "    #print('Header of raw data\\n', raw_data.head())\n",
    "    if daily == True:\n",
    "        raw_data['Time'] = pd.to_datetime(raw_data['Time'], format=\"%m/%d/%Y\")\n",
    "    else:\n",
    "        raw_data['Time'] = pd.to_datetime(raw_data['Time'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "        \n",
    "    raw_data = raw_data.set_index('Time')\n",
    "    \n",
    "    ## Storing into new Series\n",
    "    data_byFBID = pd.Series() # Series holding dataframes for each Fitbit (FB) ID\n",
    "    for i in range(len(ids)):\n",
    "        data_byFBID[str(ids[i])] = raw_data[raw_data.ID == ids[i]]\n",
    "        \n",
    "    # Converting to Series by Beiwe ID\n",
    "    df_byID = pd.Series() # Series holding dataframes for each Beiwe ID\n",
    "    for i in range(len(data_byFBID)):\n",
    "        for j in range(len(record_ids)):\n",
    "            if str(data_byFBID.index[i]) == str(record_ids[j]):\n",
    "                df_byID[str(beiwe_ids[j])] = data_byFBID.iloc[i]\n",
    "\n",
    "    # Removing data from DF that isn't in the deployment range\n",
    "    for name in df_byID.index:\n",
    "        ## Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ## Looping through all values read in\n",
    "        for j in range(len(df_byID[name])):\n",
    "            if df_byID[name].index[j].month == start_date.month and df_byID[name].index[j].day == start_date.day:\n",
    "                ### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "        ## Removing the excess 'head' of data\n",
    "        df_byID[name] = df_byID[name][start_index:]\n",
    "\n",
    "        ## Looping through all values read in (minus the part we just got rid of)\n",
    "        for j in range(len(df_byID[name])):\n",
    "            if df_byID[name].index[j].month == end_date.month and df_byID[name].index[j].day == end_date.day:\n",
    "                ### Once we find the end month and date, we want to stop and store the index just before this because the last day is incomplete\n",
    "                end_index = j-1\n",
    "                break\n",
    "        ## Removing the excess 'tail' of data\n",
    "        df_byID[name] = df_byID[name][0:end_index]\n",
    "        \n",
    "    return df_byID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beiwe Data\n",
    "Used for import of:\n",
    "- GPS\n",
    "- Accelerometer\n",
    "- Power State\n",
    "- Reachability\n",
    "- Survey Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beiweImport(deployment_info, beiwe_ids, var):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - deployment_info: a list of strings that contains the semester, deployment tag, deployment number,\n",
    "            starting date, and ending date of deployment\n",
    "        - beiwe_ids: a list of string containing the beiwe IDs from this deployment's data set\n",
    "        - var: string of the variable's directory name\n",
    "    Returns\n",
    "    '''\n",
    "    # Reading in the file\n",
    "    df_all = pd.DataFrame() # dataframe that combines all the data from every participant\n",
    "    df_byID = pd.Series() # series that stores dataframes for each individual\n",
    "    for name in beiwe_ids:\n",
    "        DIR = deployment_info[0] + '/' + deployment_info[1] + '/Beiwe_Data/'\n",
    "        if var in os.listdir(DIR + str(name)):\n",
    "            DIR = DIR + name + '/' + var + '/'\n",
    "            raw_data = pd.DataFrame() # resets on each name\n",
    "            for file in os.listdir(DIR):\n",
    "                temp = pd.read_csv(DIR + file) # data from single file\n",
    "                raw_data = pd.concat([raw_data,temp], sort=False) # data from individual\n",
    "                df_all = pd.concat([df_all,temp], sort=False) # never resets\n",
    "            df_byID[name] = raw_data # Storing the data from each individual in the overall series\n",
    "        else:\n",
    "            print('No', var, 'data found for ID', name)\n",
    "            \n",
    "    print(var.upper(), 'DATA IMPORTED')\n",
    "    \n",
    "    return df_byID, df_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
