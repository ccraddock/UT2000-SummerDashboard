{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Function Module\n",
    "This notebook includes all the functions needed for the **lambda operations**. Each of these functions performs three operations:\n",
    "1. Imports the data\n",
    "2. Cleans and/or reorganizes the data in a usable way\n",
    "3. Write the data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conditions\n",
    "A CSV file is generated for each user that has the following data\n",
    "1. Timestamp\n",
    "2. Hourly-averaged temperatures\n",
    "3. Hourly-averaged relative humidity\n",
    "\n",
    "In addition, one file denoted *Aggregate_ThermalConditions.csv* holds the same hourly temperature and relative humdity profiles for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaThermalConditions(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns two dataframes, one containing hourly T/RH values per ID and another containing aggregate hourly T/RH\n",
    "    '''\n",
    "    raw_data = pd.DataFrame()\n",
    "    id_list = []\n",
    "    # Importing the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            id_list.append(folder)\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/beacon_data/bevo/sht31d/' # Location of file\n",
    "            temp = pd.DataFrame() # Stores one csv file's worth of data\n",
    "\n",
    "            ## Looping through all the files in the sensor directory\n",
    "            for file in os.listdir(DIR):\n",
    "                if str(file[-3:]) == 'csv': # To ensure that we only read in csv files\n",
    "                    temp = pd.read_csv(DIR + file,header=None,names=['Time','RH','Temperature_C'])\n",
    "                    temp['ID'] = folder\n",
    "                    raw_data = pd.concat([raw_data,temp],axis=0,ignore_index=True)\n",
    "\n",
    "    ## Creating a date array for indexing that converts utctimestamp to Central Time\n",
    "    raw_data = raw_data.dropna() # Dropping any NaNs\n",
    "    t = np.zeros((len(raw_data)),dtype='datetime64[ns]') # Array to store times\n",
    "    for j in range(len(t)):\n",
    "        ts = int(raw_data['Time'].values[j])\n",
    "        t[j] = datetime.strptime(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S') - timedelta(hours=5)\n",
    "\n",
    "    ## Re-indexing and re-naming\n",
    "    raw_data['Time'] = t\n",
    "    raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "    raw_data = raw_data.sort_index()\n",
    "\n",
    "    ## Adding column for temperature in Farenheit\n",
    "    raw_data['Temperature_F'] = raw_data['Temperature_C']*1.8+32\n",
    "\n",
    "    ## Removing data from DF that isn't in the deployment range\n",
    "    start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "    end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "    ### Checking to see if there is data in the range\n",
    "    if raw_data.index[-1] < start_date:\n",
    "        print('\\tNo data from this deployment range')\n",
    "\n",
    "    ## Checking to see if we are importing one day's worth of data\n",
    "    elif start_date == end_date:\n",
    "        raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month\n",
    "        raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "    ## Normal range of data\n",
    "    else:\n",
    "        ### Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ### Looping through to find the start dates\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                #### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "            if raw_data.index[j] > start_date:\n",
    "                #### In the rare case we tried to import a day that is not present in the dataset, we have to find the next closest\n",
    "                start_index = j\n",
    "                break\n",
    "        ### Removing the data gathered before the start index/start date\n",
    "        raw_data = raw_data[start_index:]\n",
    "\n",
    "        ### Looping through remaining values to find the end date\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j] > end_date:\n",
    "                end_index = j-1\n",
    "                break\n",
    "\n",
    "        ### Removing any data that remains after the ending index/end date\n",
    "        raw_data = raw_data[0:end_index]\n",
    "        \n",
    "    # Getting new columns to group by\n",
    "    raw_data['Month'] = raw_data.index.month\n",
    "    raw_data['Day'] = raw_data.index.day\n",
    "    raw_data['Hour'] = raw_data.index.hour\n",
    "    \n",
    "    # Writing Out Aggregate Data\n",
    "    hourly_mean = raw_data.groupby(['Month','Day','Hour']).mean() # Mean for each hour\n",
    "    hourly_count = raw_data.groupby(['Month','Day','Hour']).count()\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(hourly_mean)):\n",
    "        dates.append(datetime(2019,hourly_mean.index[i][0],hourly_mean.index[i][1],hourly_mean.index[i][2]))\n",
    "    hourly_mean['Date'] = dates # Attaching new column\n",
    "    hourly_mean['Count'] = hourly_count['RH']\n",
    "    hourly_mean.to_csv('Files/Aggregate_ThermalConditions.csv')\n",
    "    \n",
    "    # Grouping by ID and Averaging Data by Hour\n",
    "    hourly_mean_byID = raw_data.groupby(['ID','Month','Day','Hour']).mean() # Mean for each hour for an ID\n",
    "    ## Converting separate date columns to single datetiem entry and writing to csv\n",
    "    for name in id_list:\n",
    "        dates = []\n",
    "        df = hourly_mean_byID.loc[name]\n",
    "        for i in range(len(df)):\n",
    "            dates.append(datetime(2019,df.index[i][0],df.index[i][1],df.index[i][2]))\n",
    "        df['Date'] = dates\n",
    "        df.to_csv('Files/' + name + '_ThermalConditions.csv')\n",
    "    \n",
    "    return hourly_mean,hourly_mean_byID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Air Quality\n",
    "A CSV file is generated for each user that has the following data\n",
    "1. Timestamp\n",
    "2. Hourly-averaged PM2.5 Concentration ($\\mu$g/m$^3$)\n",
    "3. Hourly-average PM2.5 AQI (0-500)\n",
    "\n",
    "In addition, one file denoted *Aggregate_IAQ.csv* holds the same hourly temperature and relative humdity profiles for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaIAQ(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns two dataframes, one containing hourly PM2.5 values per ID and another containing aggregate hourly PM2.5\n",
    "    '''\n",
    "    raw_data = pd.DataFrame()\n",
    "    id_list = []\n",
    "    # Importing the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            id_list.append(folder)\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/beacon_data/bevo/pms5003/' # Location of file\n",
    "            temp = pd.DataFrame() # Stores one csv file's worth of data\n",
    "\n",
    "            ## Looping through all the files in the sensor directory\n",
    "            for file in os.listdir(DIR):\n",
    "                if str(file[-3:]) == 'csv': # To ensure that we only read in csv files\n",
    "                    temp = pd.read_csv(DIR + file,header=None,names=['Time','Concentration'],usecols=[0,2])\n",
    "                    temp['ID'] = folder\n",
    "                    raw_data = pd.concat([raw_data,temp],axis=0,ignore_index=True)\n",
    "\n",
    "    ## Creating a date array for indexing that converts utctimestamp to Central Time\n",
    "    raw_data = raw_data.dropna() # Dropping any NaNs\n",
    "    t = np.zeros((len(raw_data)),dtype='datetime64[ns]') # Array to store times\n",
    "    for j in range(len(t)):\n",
    "        ts = int(raw_data['Time'].values[j])\n",
    "        t[j] = datetime.strptime(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S') - timedelta(hours=5)\n",
    "\n",
    "    ## Re-indexing and re-naming\n",
    "    raw_data['Time'] = t\n",
    "    raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "    raw_data = raw_data.sort_index()\n",
    "\n",
    "    ## Adding column for PM2.5 in AQI\n",
    "    raw_data['AQI'] = getAQI(raw_data['Concentration'])\n",
    "\n",
    "    ## Removing data from DF that isn't in the deployment range\n",
    "    start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "    end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "    ### Checking to see if there is data in the range\n",
    "    if raw_data.index[-1] < start_date:\n",
    "        print('\\tNo data from this deployment range')\n",
    "\n",
    "    ## Checking to see if we are importing one day's worth of data\n",
    "    elif start_date == end_date:\n",
    "        raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month\n",
    "        raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "    ## Normal range of data\n",
    "    else:\n",
    "        ### Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ### Looping through to find the start dates\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                #### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "            if raw_data.index[j] > start_date:\n",
    "                #### In the rare case we tried to import a day that is not present in the dataset, we have to find the next closest\n",
    "                start_index = j\n",
    "                break\n",
    "        ### Removing the data gathered before the start index/start date\n",
    "        raw_data = raw_data[start_index:]\n",
    "\n",
    "        ### Looping through remaining values to find the end date\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j] > end_date:\n",
    "                end_index = j-1\n",
    "                break\n",
    "\n",
    "        ### Removing any data that remains after the ending index/end date\n",
    "        raw_data = raw_data[0:end_index]\n",
    "        \n",
    "    # Getting new columns to group by\n",
    "    raw_data['Month'] = raw_data.index.month\n",
    "    raw_data['Day'] = raw_data.index.day\n",
    "    raw_data['Hour'] = raw_data.index.hour\n",
    "    \n",
    "    # Writing Out Aggregate Data\n",
    "    hourly_mean = raw_data.groupby(['Month','Day','Hour']).mean() # Mean for each hour\n",
    "    hourly_count = raw_data.groupby(['Month','Day','Hour']).count() # Count for each hour\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(hourly_mean)):\n",
    "        dates.append(datetime(2019,hourly_mean.index[i][0],hourly_mean.index[i][1],hourly_mean.index[i][2]))\n",
    "    hourly_mean['Date'] = dates # Attaching new column\n",
    "    hourly_mean['Count'] = hourly_count['Concentration']\n",
    "    hourly_mean.to_csv('Files/Aggregate_IAQ.csv')\n",
    "    \n",
    "    # Grouping by ID and Averaging Data by Hour\n",
    "    hourly_mean_byID = raw_data.groupby(['ID','Month','Day','Hour']).mean() # Mean for each hour for an ID\n",
    "    ## Converting separate date columns to single datetiem entry and writing to csv\n",
    "    for name in id_list:\n",
    "        dates = []\n",
    "        df = hourly_mean_byID.loc[name]\n",
    "        for i in range(len(df)):\n",
    "            dates.append(datetime(2019,df.index[i][0],df.index[i][1],df.index[i][2]))\n",
    "        df['Date'] = dates\n",
    "        df.to_csv('Files/' + name + '_IAQ.csv')\n",
    "    \n",
    "    return hourly_mean,hourly_mean_byID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAQI(concentration):\n",
    "    '''\n",
    "    Input:\n",
    "        - concentration: numpy float array holding the PM2.5 concentrations in ug/m^3\n",
    "    Returns the PM2.5 concentration as air quality index\n",
    "    '''\n",
    "    aqi = []\n",
    "    for C in concentration:\n",
    "        if C <= 12.0:\n",
    "            aqi_score = round(C/12.0 * 50.0)\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 35.4:\n",
    "            aqi_score = round(50 + (C-12.1)/(35.4-12.1) * (100-50))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 55.4:\n",
    "            aqi_score = round(100 + (C-35.5)/(55.4-35.5) * (150-100))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 150.4:\n",
    "            aqi_score = round(150 + (C-55.5)/(150.4-55.5) * (200-150))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 250.4:\n",
    "            aqi_score = round(200 + (C-150.5)/(250.4-150.5) * (300-200))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 350.4:\n",
    "            aqi_score = round(300 + (C-250.5)/(350.4-250.5) * (400-300))\n",
    "            aqi.append(aqi_score)\n",
    "        else:\n",
    "            aqi_score = round(400 + (C-350.5)/(500.4-350.5) * (500-400))\n",
    "            aqi.append(aqi_score)\n",
    "            \n",
    "    return aqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitbit Sleep Quality\n",
    "A CSV file is generated for each user that has the following data\n",
    "1. Night (Timestamp)\n",
    "2. Percent Awake\n",
    "3. Percent REM\n",
    "4. Percent Non-REM\n",
    "6. Sleep Latency\n",
    "6. Sleep Efficiency as a Percentage\n",
    "7. Sleep Efficiency as a Letter Grade\n",
    "\n",
    "In addition, one file denoted *Aggregate_FitbitSQ.csv* holds the same values but aggregated over all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSleepStages(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - ID: string representing the ID number\n",
    "        - file_name: a string that contains the csv file we are looking for\n",
    "        - name_list: a list of strings to use as the column labels in the dataframe\n",
    "        - column_list: a list of numbers that correspond to the columns to be imported, default is 'all'\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns a dataframe containing the timestamp and the measured variables that correspond to the file_name variable\n",
    "    '''\n",
    "    stages_byID = pd.Series()\n",
    "    # Importing and cleaning the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/' # Location of file\n",
    "            try:\n",
    "                raw_data = pd.read_csv(DIR + 'SleepStages.csv',header=0,names=['Time','ShortWakes','Stage_Label'],usecols=[1,3,4])\n",
    "                ## Converting the time column to datetime\n",
    "                raw_data['Time'] = pd.to_datetime(raw_data['Time'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "                raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "                raw_data = raw_data.sort_index()\n",
    "\n",
    "                # Removing data from DF that isn't in the deployment range\n",
    "                start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "                end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "                ## Checking to see if there is data in the range\n",
    "                if raw_data.index[-1] < start_date:\n",
    "                    print('\\tNo data from this deployment range')\n",
    "                    return df\n",
    "                ## Checking to see if we are importing one day's worth of data\n",
    "                elif start_date == end_date:\n",
    "                    raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month to ensure only one day\n",
    "                    raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "                    # Storing the cleaned data to the final dataframe\n",
    "                    print('\\tNumber of datapoints: ' + str(len(raw_data)))\n",
    "                    df = raw_data\n",
    "\n",
    "                    # Returning dataframe with cleaned data\n",
    "                    return df\n",
    "                else:\n",
    "                    ## Variables to store the correct indexes\n",
    "                    start_index = 0\n",
    "                    end_index = -1\n",
    "                    ## Looping through all values read in\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                            ### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                            start_index = j\n",
    "                            break\n",
    "                        if raw_data.index[j] > start_date:\n",
    "                            ### In the rare case we tried to import a day that is not present in the dataset, we have to fine the next closest\n",
    "                            start_index = j\n",
    "                            break\n",
    "\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[start_index:]\n",
    "\n",
    "                    ## Looping through the remaining values\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j] > end_date:\n",
    "                            end_index = j-1\n",
    "                            break\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[0:end_index]\n",
    "                    \n",
    "                    stages_byID[folder] = raw_data\n",
    "        \n",
    "            except FileNotFoundError:\n",
    "                print('No data available for this ID')\n",
    "                \n",
    "    # Getting the Sleep Metrics\n",
    "    sleep_metrics = pd.DataFrame()\n",
    "    sleep_metrics_byID = pd.Series()\n",
    "    for name in stages_byID.index:\n",
    "        ## Relabeling\n",
    "        sleep_stages = stages_byID[name]\n",
    "        stages = sleep_stages['Stage_Label']\n",
    "        times = sleep_stages.index\n",
    "        \n",
    "        ## Getting the different sleep times\n",
    "        locs = []\n",
    "        for i in range(len(sleep_stages)-1):\n",
    "            # Parsing out the days by looking for timesteps greater than 5 minutes\n",
    "            if sleep_stages.index[i+1]-sleep_stages.index[i] > timedelta(seconds = 300):\n",
    "                locs.append(i+1)\n",
    "\n",
    "        stages_byDay = np.split(stages,locs)\n",
    "        times_byDay = np.split(times,locs)\n",
    "\n",
    "        latency = []\n",
    "        efficiency = []\n",
    "        grade = []\n",
    "        night = []\n",
    "        time_asleep = []\n",
    "        awake_percentage = []\n",
    "        rem_percentage = []\n",
    "        nonrem_percentage = []\n",
    "\n",
    "        for i in range(len(stages_byDay)):\n",
    "            # Checking to see if the person was in bed for at least 2 hours (120 30-second periods)\n",
    "            if len(stages_byDay[i]) > 119:\n",
    "                night.append(datetime.strptime(str(times_byDay[i][0])[0:10],'%Y-%m-%d'))\n",
    "                time_asleep.append(len(stages_byDay[i])*30/60/60)\n",
    "                n = 0\n",
    "                while stages_byDay[i][n] == 'wake':\n",
    "                    n += 1\n",
    "\n",
    "                latency.append((n*30)/60/60)\n",
    "                wake_count = 0\n",
    "                rem_count = 0\n",
    "                nonrem_count = 0\n",
    "                for j in range(len(stages_byDay[i])):\n",
    "                    if stages_byDay[i][j] == 'wake':\n",
    "                        wake_count += 1\n",
    "                    elif stages_byDay[i][j] == 'rem':\n",
    "                        rem_count += 1\n",
    "                    else:\n",
    "                        nonrem_count += 1\n",
    "\n",
    "                efficiency.append((1 - wake_count/len(stages_byDay[i]))*100)\n",
    "                awake_percentage.append(wake_count/len(stages_byDay[i])*100)\n",
    "                rem_percentage.append(rem_count/len(stages_byDay[i])*100)\n",
    "                nonrem_percentage.append(nonrem_count/len(stages_byDay[i])*100)\n",
    "                if efficiency[-1] >= 90:\n",
    "                    grade.append('A')\n",
    "                elif efficiency[-1] < 90 and efficiency[-1] >= 85:\n",
    "                    grade.append('B')\n",
    "                elif efficiency[-1] < 85 and efficiency[-1] >= 80:\n",
    "                    grade.append('C')\n",
    "                else:\n",
    "                    grade.append('F')\n",
    "\n",
    "        d = {'Night': night, \n",
    "             'Time_Asleep': time_asleep,\n",
    "             '%Awake': awake_percentage,\n",
    "             '%REM': rem_percentage,\n",
    "             '%Non-REM': nonrem_percentage,\n",
    "             'Latency': latency,\n",
    "             'Efficiency': efficiency,\n",
    "             'Efficiency_Grade': grade}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        # Individual Data\n",
    "        sleep_metrics_byID[name] = df.set_index('Night')\n",
    "        sleep_metrics_byID[name].to_csv('Files/' + name + '_FitbitSQ.csv')\n",
    "        # Aggregate Data\n",
    "        df['ID'] = name\n",
    "        sleep_metrics = pd.concat([sleep_metrics,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    nightly_mean = sleep_metrics.groupby(['Night']).mean()\n",
    "    nightly_count = sleep_metrics.groupby(['Night']).count()\n",
    "    nightly_mean['Count'] = nightly_count['Time_Asleep']\n",
    "    nightly_mean.to_csv('Files/Aggregate_FitbitSQ.csv')\n",
    "    \n",
    "    return stages_byID, sleep_metrics_byID, nightly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beiwe Sleep Quality\n",
    "Sleep Quality from the Beiwe system can be thought of \"Perceived Sleep Quality\". The CSV file generated has the following data:\n",
    "1. Night (Timestamp)\n",
    "2. Time Asleep\n",
    "3. Restfulness Score\n",
    "4. Refresh Score\n",
    "5. Aggregate Score \n",
    "6. Weighted Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
