{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Function Module\n",
    "This notebook includes all the functions needed for the **lambda operations**. Each of these functions performs three operations:\n",
    "1. Imports the data\n",
    "2. Cleans and/or reorganizes the data in a usable way\n",
    "3. Write the data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal Conditions\n",
    "A CSV file is generated for each user that has the following data\n",
    "1. Timestamp\n",
    "2. Hourly-averaged temperatures\n",
    "3. Hourly-averaged relative humidity\n",
    "\n",
    "In addition, one file denoted *Aggregate_ThermalConditions.csv* holds the same hourly temperature and relative humdity profiles for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaThermalConditions(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns two dataframes, one containing hourly T/RH values per ID and another containing aggregate hourly T/RH\n",
    "    '''\n",
    "    raw_data = pd.DataFrame()\n",
    "    id_list = []\n",
    "    # Importing the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            id_list.append(folder)\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/beacon_data/bevo/sht31d/' # Location of file\n",
    "            temp = pd.DataFrame() # Stores one csv file's worth of data\n",
    "\n",
    "            ## Looping through all the files in the sensor directory\n",
    "            for file in os.listdir(DIR):\n",
    "                if str(file[-3:]) == 'csv': # To ensure that we only read in csv files\n",
    "                    temp = pd.read_csv(DIR + file,header=None,names=['Time','RH','Temperature_C'])\n",
    "                    temp['ID'] = folder\n",
    "                    raw_data = pd.concat([raw_data,temp],axis=0,ignore_index=True)\n",
    "\n",
    "    ## Creating a date array for indexing that converts utctimestamp to Central Time\n",
    "    raw_data = raw_data.dropna() # Dropping any NaNs\n",
    "    t = np.zeros((len(raw_data)),dtype='datetime64[ns]') # Array to store times\n",
    "    for j in range(len(t)):\n",
    "        ts = int(raw_data['Time'].values[j])\n",
    "        t[j] = datetime.strptime(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S') - timedelta(hours=5)\n",
    "\n",
    "    ## Re-indexing and re-naming\n",
    "    raw_data['Time'] = t\n",
    "    raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "    raw_data = raw_data.sort_index()\n",
    "\n",
    "    ## Adding column for temperature in Farenheit\n",
    "    raw_data['Temperature_F'] = raw_data['Temperature_C']*1.8+32\n",
    "\n",
    "    ## Removing data from DF that isn't in the deployment range\n",
    "    start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "    end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "    ### Checking to see if there is data in the range\n",
    "    if raw_data.index[-1] < start_date:\n",
    "        print('\\tNo data from this deployment range')\n",
    "\n",
    "    ## Checking to see if we are importing one day's worth of data\n",
    "    elif start_date == end_date:\n",
    "        raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month\n",
    "        raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "    ## Normal range of data\n",
    "    else:\n",
    "        ### Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ### Looping through to find the start dates\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                #### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "            if raw_data.index[j] > start_date:\n",
    "                #### In the rare case we tried to import a day that is not present in the dataset, we have to find the next closest\n",
    "                start_index = j\n",
    "                break\n",
    "        ### Removing the data gathered before the start index/start date\n",
    "        raw_data = raw_data[start_index:]\n",
    "\n",
    "        ### Looping through remaining values to find the end date\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j] > end_date:\n",
    "                end_index = j-1\n",
    "                break\n",
    "\n",
    "        ### Removing any data that remains after the ending index/end date\n",
    "        raw_data = raw_data[0:end_index]\n",
    "        \n",
    "    # Getting new columns to group by\n",
    "    raw_data['Month'] = raw_data.index.month\n",
    "    raw_data['Day'] = raw_data.index.day\n",
    "    raw_data['Hour'] = raw_data.index.hour\n",
    "    \n",
    "    # Writing Out Aggregate Data\n",
    "    hourly_mean = raw_data.groupby(['Month','Day','Hour']).mean() # Mean for each hour\n",
    "    hourly_count = raw_data.groupby(['Month','Day','Hour']).count()\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(hourly_mean)):\n",
    "        dates.append(datetime(2019,hourly_mean.index[i][0],hourly_mean.index[i][1],hourly_mean.index[i][2]))\n",
    "    hourly_mean['Date'] = dates # Attaching new column\n",
    "    hourly_mean['Count'] = hourly_count['RH']\n",
    "    hourly_mean.to_csv('Files/Aggregate_ThermalConditions.csv')\n",
    "    \n",
    "    # Grouping by ID and Averaging Data by Hour\n",
    "    hourly_mean_byID = raw_data.groupby(['ID','Month','Day','Hour']).mean() # Mean for each hour for an ID\n",
    "    ## Converting separate date columns to single datetiem entry and writing to csv\n",
    "    for name in id_list:\n",
    "        dates = []\n",
    "        df = hourly_mean_byID.loc[name]\n",
    "        for i in range(len(df)):\n",
    "            dates.append(datetime(2019,df.index[i][0],df.index[i][1],df.index[i][2]))\n",
    "        df['Date'] = dates\n",
    "        df.to_csv('Files/' + name + '_ThermalConditions.csv')\n",
    "    \n",
    "    return hourly_mean,hourly_mean_byID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Air Quality\n",
    "Two sets of CSV files are generated, one for hourly data and another for nightly metrics like median and peak concentrations.\n",
    "\n",
    "### Hourly\n",
    "A CSV file is generated for each user that has the following data:\n",
    "1. Timestamp\n",
    "2. Hourly-averaged PM2.5 Concentration ($\\mu$g/m$^3$)\n",
    "3. Hourly-average PM2.5 AQI (0-500)\n",
    "\n",
    "In addition, one file denoted *Aggregate_IAQ.csv* holds the same hourly temperature and relative humdity profiles for all users.\n",
    "\n",
    "### Daily\n",
    "A CSV file is generated for each user that has the following data:\n",
    "1. Night\n",
    "2. Median Nightly Concentration\n",
    "3. Peak Nightly Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaIAQ(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns two dataframes, one containing hourly PM2.5 values per ID and another containing aggregate hourly PM2.5\n",
    "    '''\n",
    "    raw_data = pd.DataFrame()\n",
    "    id_list = []\n",
    "    # Importing the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            id_list.append(folder)\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/beacon_data/bevo/pms5003/' # Location of file\n",
    "            temp = pd.DataFrame() # Stores one csv file's worth of data\n",
    "\n",
    "            ## Looping through all the files in the sensor directory\n",
    "            for file in os.listdir(DIR):\n",
    "                if str(file[-3:]) == 'csv': # To ensure that we only read in csv files\n",
    "                    temp = pd.read_csv(DIR + file,header=None,names=['Time','Concentration'],usecols=[0,2])\n",
    "                    temp['ID'] = folder\n",
    "                    raw_data = pd.concat([raw_data,temp],axis=0,ignore_index=True)\n",
    "\n",
    "    ## Creating a date array for indexing that converts utctimestamp to Central Time\n",
    "    raw_data = raw_data.dropna() # Dropping any NaNs\n",
    "    t = np.zeros((len(raw_data)),dtype='datetime64[ns]') # Array to store times\n",
    "    for j in range(len(t)):\n",
    "        ts = int(raw_data['Time'].values[j])\n",
    "        t[j] = datetime.strptime(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'),'%Y-%m-%d %H:%M:%S') - timedelta(hours=5)\n",
    "\n",
    "    ## Re-indexing and re-naming\n",
    "    raw_data['Time'] = t\n",
    "    raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "    raw_data = raw_data.sort_index()\n",
    "\n",
    "    ## Adding column for PM2.5 in AQI\n",
    "    raw_data['AQI'] = getAQI(raw_data['Concentration'])\n",
    "\n",
    "    ## Removing data from DF that isn't in the deployment range\n",
    "    start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "    end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "    ### Checking to see if there is data in the range\n",
    "    if raw_data.index[-1] < start_date:\n",
    "        print('\\tNo data from this deployment range')\n",
    "\n",
    "    ## Checking to see if we are importing one day's worth of data\n",
    "    elif start_date == end_date:\n",
    "        raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month\n",
    "        raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "    ## Normal range of data\n",
    "    else:\n",
    "        ### Variables to store the correct indexes\n",
    "        start_index = 0\n",
    "        end_index = -1\n",
    "        ### Looping through to find the start dates\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                #### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                start_index = j\n",
    "                break\n",
    "            if raw_data.index[j] > start_date:\n",
    "                #### In the rare case we tried to import a day that is not present in the dataset, we have to find the next closest\n",
    "                start_index = j\n",
    "                break\n",
    "        ### Removing the data gathered before the start index/start date\n",
    "        raw_data = raw_data[start_index:]\n",
    "\n",
    "        ### Looping through remaining values to find the end date\n",
    "        for j in range(len(raw_data)):\n",
    "            if raw_data.index[j] > end_date:\n",
    "                end_index = j-1\n",
    "                break\n",
    "\n",
    "        ### Removing any data that remains after the ending index/end date\n",
    "        raw_data = raw_data[0:end_index]\n",
    "        \n",
    "    # Getting new columns to group by\n",
    "    raw_data['Month'] = raw_data.index.month\n",
    "    raw_data['Day'] = raw_data.index.day\n",
    "    raw_data['Hour'] = raw_data.index.hour\n",
    "    \n",
    "    # Writing Out Aggregate Data\n",
    "    hourly_mean = raw_data.groupby(['Month','Day','Hour']).mean() # Mean for each hour\n",
    "    hourly_count = raw_data.groupby(['Month','Day','Hour']).count() # Count for each hour\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(hourly_mean)):\n",
    "        dates.append(datetime(2019,hourly_mean.index[i][0],hourly_mean.index[i][1],hourly_mean.index[i][2]))\n",
    "    hourly_mean['Date'] = dates # Attaching new column\n",
    "    hourly_mean['Count'] = hourly_count['Concentration']\n",
    "    hourly_mean.to_csv('Files/Aggregate_IAQ.csv')\n",
    "    \n",
    "    # Grouping by ID and Averaging Data by Hour\n",
    "    hourly_mean_byID = raw_data.groupby(['ID','Month','Day','Hour']).mean() # Mean for each hour for an ID\n",
    "    ## Converting separate date columns to single datetime entry and writing to csv\n",
    "    concentration_byID = pd.Series()\n",
    "    for name in id_list:\n",
    "        dates = []\n",
    "        df = hourly_mean_byID.loc[name]\n",
    "        for i in range(len(df)):\n",
    "            dates.append(datetime(2019,df.index[i][0],df.index[i][1],df.index[i][2]))\n",
    "        df['Date'] = dates\n",
    "        concentration_byID[name] = df\n",
    "        df.to_csv('Files/' + name + '_IAQ.csv')\n",
    "        \n",
    "    # Importing Sleep Stage Data\n",
    "    stages_byID = pd.Series()\n",
    "    # Importing and cleaning the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/' # Location of file\n",
    "            try:\n",
    "                raw_data = pd.read_csv(DIR + 'SleepStages.csv',header=0,names=['Time','ShortWakes','Stage_Label'],usecols=[1,3,4])\n",
    "                ## Converting the time column to datetime\n",
    "                raw_data['Time'] = pd.to_datetime(raw_data['Time'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "                raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "                raw_data = raw_data.sort_index()\n",
    "\n",
    "                # Removing data from DF that isn't in the deployment range\n",
    "                start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "                end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "                ## Checking to see if there is data in the range\n",
    "                if raw_data.index[-1] < start_date:\n",
    "                    print('\\tNo data from this deployment range')\n",
    "                    return df\n",
    "                ## Checking to see if we are importing one day's worth of data\n",
    "                elif start_date == end_date:\n",
    "                    raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month to ensure only one day\n",
    "                    raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "                    # Storing the cleaned data to the final dataframe\n",
    "                    print('\\tNumber of datapoints: ' + str(len(raw_data)))\n",
    "                    df = raw_data\n",
    "\n",
    "                    # Returning dataframe with cleaned data\n",
    "                    return df\n",
    "                else:\n",
    "                    ## Variables to store the correct indexes\n",
    "                    start_index = 0\n",
    "                    end_index = -1\n",
    "                    ## Looping through all values read in\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                            ### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                            start_index = j\n",
    "                            break\n",
    "                        if raw_data.index[j] > start_date:\n",
    "                            ### In the rare case we tried to import a day that is not present in the dataset, we have to fine the next closest\n",
    "                            start_index = j\n",
    "                            break\n",
    "\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[start_index:]\n",
    "\n",
    "                    ## Looping through the remaining values\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j] > end_date:\n",
    "                            end_index = j-1\n",
    "                            break\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[0:end_index]\n",
    "                    \n",
    "                    stages_byID[folder] = raw_data\n",
    "        \n",
    "            except FileNotFoundError:\n",
    "                print('No data available for this ID')\n",
    "    \n",
    "    # Getting concentration metrics for each day\n",
    "    hourly_sleepconcentration_byID = pd.Series()\n",
    "    nightly_concentration_byID = pd.Series()\n",
    "    for name in stages_byID.index:\n",
    "        if name in concentration_byID.index:\n",
    "            concentration = concentration_byID[name]\n",
    "            short_date = []\n",
    "            for i in range(len(stages_byID[name])):\n",
    "                short_date.append(str(stages_byID[name].index[i])[0:16])\n",
    "            stages_byID[name]['short_date'] = short_date\n",
    "\n",
    "            short_date = []\n",
    "            for i in range(len(concentration)):\n",
    "                short_date.append(str(concentration['Date'][i])[0:16])\n",
    "            concentration['short_date'] = short_date \n",
    "\n",
    "            sleep_concentration = stages_byID[name].merge(concentration,left_on='short_date',right_on='short_date')\n",
    "            \n",
    "            times = []\n",
    "            for i in range(len(sleep_concentration)):\n",
    "                times.append(datetime.strptime(sleep_concentration['short_date'][i],'%Y-%m-%d %H:%M'))\n",
    "\n",
    "            # Creating a datetime index\n",
    "            sleep_concentration['Time'] = times\n",
    "            sleep_concentration = sleep_concentration.set_index('Time')\n",
    "            sleep_concentration.drop('short_date',axis=1)\n",
    "            if len(sleep_concentration) > 0:\n",
    "                hourly_sleepconcentration_byID[name] = sleep_concentration\n",
    "\n",
    "    nightly_concentration = pd.DataFrame()\n",
    "    ## Individual data\n",
    "    for name in hourly_sleepconcentration_byID.index:\n",
    "        sleep_concentration = hourly_sleepconcentration_byID[name]\n",
    "        times = sleep_concentration.index\n",
    "        locs = []\n",
    "        for i in range(len(sleep_concentration)-1):\n",
    "            if sleep_concentration.index[i+1]-sleep_concentration.index[i] > timedelta(hours = 3):\n",
    "                locs.append(i+1) \n",
    "        \n",
    "        concentration_byDay = np.split(sleep_concentration['AQI'],locs)\n",
    "        times_byDay = np.split(times,locs)\n",
    "\n",
    "        night = []\n",
    "        peaks = []\n",
    "        medians = []\n",
    "\n",
    "        for i in range(len(concentration_byDay)):\n",
    "            night.append(datetime.strptime(str(times_byDay[i][0])[0:10],'%Y-%m-%d'))\n",
    "            peaks.append(max(concentration_byDay[i]))\n",
    "            medians.append(concentration_byDay[i].median())\n",
    "\n",
    "        d = {'Night': night, 'Peak': peaks, 'Median': medians}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df = df.set_index('Night')\n",
    "        df.to_csv('Files/' + name + '_SleepIAQ.csv')\n",
    "        nightly_concentration_byID[name] = df\n",
    "        nightly_concentration = pd.concat([nightly_concentration,df],axis=0)\n",
    "    \n",
    "    ## Aggregate Data\n",
    "    nightly_concentration['Month'] = nightly_concentration.index.month\n",
    "    nightly_concentration['Day'] = nightly_concentration.index.day\n",
    "    nightly_mean = nightly_concentration.groupby(['Month','Day']).mean() # Mean for each hour\n",
    "    nightly_count = nightly_concentration.groupby(['Month','Day']).count() # Count for each hour\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(nightly_mean)):\n",
    "        dates.append(datetime(2019,nightly_mean.index[i][0],nightly_mean.index[i][1]))\n",
    "    nightly_mean['Date'] = dates # Attaching new column\n",
    "    nightly_mean['Count'] = nightly_count['Peak']\n",
    "    nightly_mean.to_csv('Files/Aggregate_SleepIAQ.csv')\n",
    "    \n",
    "    return hourly_mean,hourly_mean_byID,hourly_sleepconcentration_byID,nightly_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAQI(concentration):\n",
    "    '''\n",
    "    Input:\n",
    "        - concentration: numpy float array holding the PM2.5 concentrations in ug/m^3\n",
    "    Returns the PM2.5 concentration as air quality index\n",
    "    '''\n",
    "    aqi = []\n",
    "    for C in concentration:\n",
    "        if C <= 12.0:\n",
    "            aqi_score = round(C/12.0 * 50.0)\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 35.4:\n",
    "            aqi_score = round(50 + (C-12.1)/(35.4-12.1) * (100-50))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 55.4:\n",
    "            aqi_score = round(100 + (C-35.5)/(55.4-35.5) * (150-100))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 150.4:\n",
    "            aqi_score = round(150 + (C-55.5)/(150.4-55.5) * (200-150))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 250.4:\n",
    "            aqi_score = round(200 + (C-150.5)/(250.4-150.5) * (300-200))\n",
    "            aqi.append(aqi_score)\n",
    "        elif C <= 350.4:\n",
    "            aqi_score = round(300 + (C-250.5)/(350.4-250.5) * (400-300))\n",
    "            aqi.append(aqi_score)\n",
    "        else:\n",
    "            aqi_score = round(400 + (C-350.5)/(500.4-350.5) * (500-400))\n",
    "            aqi.append(aqi_score)\n",
    "            \n",
    "    return aqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitbit Sleep Quality\n",
    "A CSV file is generated for each user that has the following data\n",
    "1. Night (Timestamp)\n",
    "2. Percent Awake\n",
    "3. Percent REM\n",
    "4. Percent Non-REM\n",
    "6. Sleep Latency\n",
    "6. Sleep Efficiency as a Percentage\n",
    "7. Sleep Efficiency as a Letter Grade\n",
    "\n",
    "In addition, one file denoted *Aggregate_FitbitSQ.csv* holds the same values but aggregated over all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSleepStages(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns a dataframe containing the timestamp and the measured variables that correspond to the file_name variable\n",
    "    '''\n",
    "    stages_byID = pd.Series()\n",
    "    # Importing and cleaning the data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            ## Important variables\n",
    "            DIR = 'Data/' + folder + '/' # Location of file\n",
    "            try:\n",
    "                raw_data = pd.read_csv(DIR + 'SleepStages.csv',header=0,names=['Time','ShortWakes','Stage_Label'],usecols=[1,3,4])\n",
    "                ## Converting the time column to datetime\n",
    "                raw_data['Time'] = pd.to_datetime(raw_data['Time'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "                raw_data = raw_data.set_index('Time') # Setting time as the dataframe index\n",
    "                raw_data = raw_data.sort_index()\n",
    "\n",
    "                # Removing data from DF that isn't in the deployment range\n",
    "                start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "                end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "                ## Checking to see if there is data in the range\n",
    "                if raw_data.index[-1] < start_date:\n",
    "                    print('\\tNo data from this deployment range')\n",
    "                    return df\n",
    "                ## Checking to see if we are importing one day's worth of data\n",
    "                elif start_date == end_date:\n",
    "                    raw_data = raw_data[raw_data.index.month == start_date.month] # mask by month to ensure only one day\n",
    "                    raw_data = raw_data[raw_data.index.day == start_date.day] # mask by the day\n",
    "\n",
    "                    # Storing the cleaned data to the final dataframe\n",
    "                    print('\\tNumber of datapoints: ' + str(len(raw_data)))\n",
    "                    df = raw_data\n",
    "\n",
    "                    # Returning dataframe with cleaned data\n",
    "                    return df\n",
    "                else:\n",
    "                    ## Variables to store the correct indexes\n",
    "                    start_index = 0\n",
    "                    end_index = -1\n",
    "                    ## Looping through all values read in\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j].month == start_date.month and raw_data.index[j].day == start_date.day:\n",
    "                            ### Once we find the month and date, we want to break so that we store the first entry from that day\n",
    "                            start_index = j\n",
    "                            break\n",
    "                        if raw_data.index[j] > start_date:\n",
    "                            ### In the rare case we tried to import a day that is not present in the dataset, we have to fine the next closest\n",
    "                            start_index = j\n",
    "                            break\n",
    "\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[start_index:]\n",
    "\n",
    "                    ## Looping through the remaining values\n",
    "                    for j in range(len(raw_data)):\n",
    "                        if raw_data.index[j] > end_date:\n",
    "                            end_index = j-1\n",
    "                            break\n",
    "                    ## Removing the data gathered before the start index/start date\n",
    "                    raw_data = raw_data[0:end_index]\n",
    "                    \n",
    "                    stages_byID[folder] = raw_data\n",
    "        \n",
    "            except FileNotFoundError:\n",
    "                print('No data available for this ID')\n",
    "                \n",
    "    # Getting the Sleep Metrics\n",
    "    sleep_metrics = pd.DataFrame()\n",
    "    sleep_metrics_byID = pd.Series()\n",
    "    for name in stages_byID.index:\n",
    "        ## Relabeling\n",
    "        sleep_stages = stages_byID[name]\n",
    "        stages = sleep_stages['Stage_Label']\n",
    "        times = sleep_stages.index\n",
    "        \n",
    "        ## Getting the different sleep times\n",
    "        locs = []\n",
    "        for i in range(len(sleep_stages)-1):\n",
    "            # Parsing out the days by looking for timesteps greater than 5 minutes\n",
    "            if sleep_stages.index[i+1]-sleep_stages.index[i] > timedelta(seconds = 300):\n",
    "                locs.append(i+1)\n",
    "\n",
    "        stages_byDay = np.split(stages,locs)\n",
    "        times_byDay = np.split(times,locs)\n",
    "\n",
    "        latency = []\n",
    "        efficiency = []\n",
    "        grade = []\n",
    "        night = []\n",
    "        time_asleep = []\n",
    "        awake_percentage = []\n",
    "        rem_percentage = []\n",
    "        nonrem_percentage = []\n",
    "\n",
    "        for i in range(len(stages_byDay)):\n",
    "            # Checking to see if the person was in bed for at least 2 hours (120 30-second periods)\n",
    "            if len(stages_byDay[i]) > 119:\n",
    "                night.append(datetime.strptime(str(times_byDay[i][0])[0:10],'%Y-%m-%d'))\n",
    "                time_asleep.append(len(stages_byDay[i])*30/60/60)\n",
    "                n = 0\n",
    "                while stages_byDay[i][n] == 'wake':\n",
    "                    n += 1\n",
    "\n",
    "                latency.append((n*30)/60/60)\n",
    "                wake_count = 0\n",
    "                rem_count = 0\n",
    "                nonrem_count = 0\n",
    "                for j in range(len(stages_byDay[i])):\n",
    "                    if stages_byDay[i][j] == 'wake':\n",
    "                        wake_count += 1\n",
    "                    elif stages_byDay[i][j] == 'rem':\n",
    "                        rem_count += 1\n",
    "                    else:\n",
    "                        nonrem_count += 1\n",
    "\n",
    "                efficiency.append((1 - wake_count/len(stages_byDay[i]))*100)\n",
    "                awake_percentage.append(wake_count/len(stages_byDay[i])*100)\n",
    "                rem_percentage.append(rem_count/len(stages_byDay[i])*100)\n",
    "                nonrem_percentage.append(nonrem_count/len(stages_byDay[i])*100)\n",
    "                if efficiency[-1] >= 90:\n",
    "                    grade.append('A')\n",
    "                elif efficiency[-1] < 90 and efficiency[-1] >= 85:\n",
    "                    grade.append('B')\n",
    "                elif efficiency[-1] < 85 and efficiency[-1] >= 80:\n",
    "                    grade.append('C')\n",
    "                else:\n",
    "                    grade.append('F')\n",
    "\n",
    "        d = {'Night': night, \n",
    "             'Time_Asleep': time_asleep,\n",
    "             '%Awake': awake_percentage,\n",
    "             '%REM': rem_percentage,\n",
    "             '%Non-REM': nonrem_percentage,\n",
    "             'Latency': latency,\n",
    "             'Efficiency': efficiency,\n",
    "             'Efficiency_Grade': grade}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        # Individual Data\n",
    "        sleep_metrics_byID[name] = df.set_index('Night')\n",
    "        sleep_metrics_byID[name].to_csv('Files/' + name + '_FitbitSQ.csv')\n",
    "        # Aggregate Data\n",
    "        df['ID'] = name\n",
    "        sleep_metrics = pd.concat([sleep_metrics,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    nightly_mean = sleep_metrics.groupby(['Night']).mean()\n",
    "    nightly_count = sleep_metrics.groupby(['Night']).count()\n",
    "    nightly_mean['Count'] = nightly_count['Time_Asleep']\n",
    "    nightly_mean.to_csv('Files/Aggregate_FitbitSQ.csv')\n",
    "    \n",
    "    return stages_byID, sleep_metrics_byID, nightly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beiwe Sleep Quality\n",
    "Sleep Quality from the Beiwe system can be thought of \"Perceived Sleep Quality\". The CSV file generated has the following data:\n",
    "1. Night (Timestamp)\n",
    "2. Time Asleep\n",
    "3. Restfulness Score\n",
    "4. Refresh Score\n",
    "5. Aggregate Score \n",
    "6. Weighted Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaSleepSurveys(starting='03/11/2019', ending='04/15/2019'):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - starting: string representing the first date to use in the data range\n",
    "        - ending: string representing the last date to use in the data range\n",
    "    Returns dataframes for each individual's survey answers and the aggregate answers \n",
    "    '''\n",
    "    sleep_surveys_byID = pd.Series()\n",
    "    sleep_surveys = pd.DataFrame()\n",
    "    # Importing and Cleaning the Data\n",
    "    for folder in os.listdir('Data/'):\n",
    "        if folder[0] != '.':\n",
    "            DIR = 'Data/' + folder + '/beiwe_data/sleep_surveys/' # Location of file\n",
    "\n",
    "            ## Important variables\n",
    "            nights = []\n",
    "            sleep_time = []\n",
    "            restful_scores = []\n",
    "            refresh_scores = []\n",
    "            aggregate = [] # Sleep score based on summing all values\n",
    "            rr = [] # Sleep score just based on refresh and restful\n",
    "            normalized = [] # Sleep score that weights each survey question by the maximum value\n",
    "\n",
    "            ## Date Range\n",
    "            start_date = datetime.strptime(starting, '%m/%d/%Y') # converting input to datetime\n",
    "            end_date = datetime.strptime(ending, '%m/%d/%Y') # converting input to datetime\n",
    "\n",
    "            for file in os.listdir(DIR):\n",
    "                numerics = []\n",
    "                # Checking to see if the file is a csv and that date already hasn't been imported\n",
    "                if file[-3:] == 'csv':\n",
    "                    file_date = datetime.strptime(file[:10],'%Y-%m-%d')\n",
    "                    # Checking to make sure we stay in the date range\n",
    "                    if file_date > start_date and file_date <= end_date:\n",
    "                        nights.append(file_date)\n",
    "                        raw_data = pd.read_csv(DIR + file,header=None,usecols=[2,4],skiprows=1,nrows=4,names=['Question','Answer'])\n",
    "                        if raw_data['Question'][0][:4] == '9:00':\n",
    "                            ## Getting average number of hours slept\n",
    "                            if str(raw_data['Answer'][1]).upper() == 'NAN' or raw_data['Answer'][1] == 'NOT_PRESENTED':\n",
    "                                sleep_time.append(-1)\n",
    "                            elif raw_data['Answer'][1] == 0:\n",
    "                                sleep_time.append(0)\n",
    "                            else:\n",
    "                                sleep_time.append((int(raw_data['Answer'][1][0]) + int(raw_data['Answer'][1][2]))/2.0)\n",
    "                            ## Getting numeric score for restfulness\n",
    "                            if raw_data['Answer'][2] == 'Not at all restful':\n",
    "                                restful_scores.append(0)\n",
    "                            elif raw_data['Answer'][2] == 'Slightly restful':\n",
    "                                restful_scores.append(1)\n",
    "                            elif raw_data['Answer'][2] == 'Somewhat restful':\n",
    "                                restful_scores.append(2)\n",
    "                            elif raw_data['Answer'][2] == 'Very restful':\n",
    "                                restful_scores.append(3)\n",
    "                            else:\n",
    "                                restful_scores.append(-1)\n",
    "                            ## Getting numeric score for refreshedness\n",
    "                            if raw_data['Answer'][3] == 'Not at all refreshed':\n",
    "                                refresh_scores.append(0)\n",
    "                            elif raw_data['Answer'][3] == 'Slightly refreshed':\n",
    "                                refresh_scores.append(1)\n",
    "                            elif raw_data['Answer'][3] == 'Somewhat refreshed':\n",
    "                                refresh_scores.append(2)\n",
    "                            elif raw_data['Answer'][3] == 'Very refreshed':\n",
    "                                refresh_scores.append(3)\n",
    "                            else:\n",
    "                                refresh_scores.append(-1)\n",
    "                        else:\n",
    "                            sleep_time.append(int(raw_data['Answer'][0]))\n",
    "                            restful_scores.append(int(raw_data['Answer'][1]))\n",
    "                            refresh_scores.append(int(raw_data['Answer'][2]))\n",
    "                        ## Getting Sleep Scores\n",
    "                        aggregate.append(sleep_time[-1]+restful_scores[-1]+refresh_scores[-1])\n",
    "                        rr.append(restful_scores[-1]+refresh_scores[-1])\n",
    "                        ### Correcting for over-sleeping in the weighted score\n",
    "                        temp_sleep = 0\n",
    "                        if sleep_time[-1] >= 8:\n",
    "                            temp_sleep = 8\n",
    "                        else:\n",
    "                            temp_sleep = sleep_time[-1]\n",
    "                        normalized.append(restful_scores[-1]/3 + refresh_scores[-1]/3 + temp_sleep/8)\n",
    "\n",
    "            # Sorting by day and returning\n",
    "            if len(nights) > 0:\n",
    "                d = {'Night': nights, 'Time_Asleep': sleep_time, 'Restful': restful_scores, 'Refreshed': refresh_scores,\n",
    "                    'Aggregate': aggregate,'Refresh+Relax': rr, 'Normalized': normalized}\n",
    "                df = pd.DataFrame(data=d)\n",
    "                df = df.set_index('Night')\n",
    "                sleep_surveys = pd.concat([sleep_surveys,df],axis=0)\n",
    "                sleep_surveys_byID[folder] = df.sort_index()\n",
    "                sleep_surveys_byID[folder].to_csv('Files/' + folder + '_BeiweSQ.csv')\n",
    "    \n",
    "    # Getting Aggregate Data\n",
    "    sleep_surveys['Month'] = sleep_surveys.index.month\n",
    "    sleep_surveys['Day'] = sleep_surveys.index.day\n",
    "    survey_mean = sleep_surveys.groupby(['Month','Day']).mean() # Mean for each hour\n",
    "    survey_count = sleep_surveys.groupby(['Month','Day']).count() # Count for each hour\n",
    "    dates = [] # list to hold dates\n",
    "    ## Converting separate date columns to single datetime entry\n",
    "    for i in range(len(survey_mean)):\n",
    "        dates.append(datetime(2019,survey_mean.index[i][0],survey_mean.index[i][1]))\n",
    "    survey_mean['Date'] = dates # Attaching new column\n",
    "    survey_mean['Count'] = survey_count['Time_Asleep']\n",
    "    survey_mean.to_csv('Files/Aggregate_BeiweSQ.csv')\n",
    "\n",
    "    return sleep_surveys_byID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
